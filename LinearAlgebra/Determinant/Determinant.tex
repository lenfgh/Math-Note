\documentclass{article}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{amsthm}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{corollary}{Corollary}[theorem]

\theoremstyle{definition} 
\newtheorem{defi}{Definition}[section]
\newtheorem{example}{Example}[defi]
\newtheorem{exercise}{Exercise}[section]
\newtheorem{solution}{Solution}[exercise]
\newtheorem{pro}{Properties}[section]


\linespread{1.5} 


\title{Determinant}
\author{Len Fu}
\date{11.20.2024-11.27.2024}

\begin{document}

\maketitle

\begin{abstract}
    This is the note made by Len Fu during his learning progress in BIT.
    The main content is from \textit{Linear Algebra Done Right} and \textit{Linear Algebra Allenby}.
\end{abstract}

\tableofcontents
\newpage

\section{Permutation}

\subsection{N Permutation}
An n-permutation is an arrangement of all the numbers $1,2,...,n$.
The total number of n-permutations is $n!$.
\subsection{Inversion and Inversion Number}
Formally, for a sequence ${a_{n}}$ with elements 
$a_{i}$ and $a_{j}$ with $i<j$, an inversion is 
present if $a_{i}>a_{j}$.

The inversion number of a permutation is the number of inversions in it.

A permutation with an odd inversion number is called 
an odd permutation. And a permutation with an even inversion number 
is called an even permutation.

\subsection{Transposition}
Formally, a transposition is a permutation that 
exchanges two elements and leaves all others unchanged.
For example, in permutation $\tau=(1,2,3,4,5)$ the 
transposition $(1,2)$ exchanges 1 and 2.
Then the resulting permutation is $\tau=(2,1,3,4,5)$.

\subsubsection{Theorem}
A transposition changes the parity of a permutation.
\textbf{Proof:}\newline
Let $\tau = (i_{1}i_{2}...i_{j}i_{j+1}...i_{n})$, and 
we exchange $i_{j}$ and $i_{j+1}$, then the remain permutation 
$(i_{1}i_{2}...i_{j}...i_{n})$ and $(i_{1}i_{2}...i_{j+1}...i_{n})$ 
keep the same parity. But the paritr of $(i_{j}i_{j+1})$ changes,
so the total parity of $\tau$ changes.

Now consider that if the transposition is between $(i_{j}i_{k})$ like 
$(...ji_{1}i_{2}...i_{s}k...)$, 
then we first transpose s times to set j into $i_{s}$ like 
$(...i_{1}i_{2}...i_{s}jk...)$. 
And we transpose j and k 
$(...i_{1}i_{2}...i_{s}kj...)$, 
then we transpose s times to set k into $i_{1}$ like 
$(...ki_{1}i_{2}...i_{s}j...)$.
The total transposition is $2s+1$. 
So the parity of the permutation changes.

\begin{corollary}
In all n permutation, the number of even permutation 
is equal to the number of odd permutation, which is 
$\frac{n!}{2}$.
\end{corollary}

\begin{proof}
Suppose there are $s$ odd permutation, then there are
$t$ even permutation. Now transpose the first two elements 
of all even permutation, then we get $s$ odd permutation.
Then $s\leq t$, conversly, transpose the first two elements 
of all odd permutation, then we get $t$ even permutation, and 
$t\leq s$. So $s=t=\frac{n!}{2}$.
\end{proof}

\begin{corollary}
    For $$a_{i_{1}k_{1}}a_{i_{2}k_{2}}\cdots a_{i_{n}k_{n}}\ =\ a_{1j_{1}}a_{2j_{2}}\cdots\ a_{nj_{n}}$$
    the inversion number is 
    $$(-1)^{\tau(i_{1}i_{2}\cdots i_{n})+\tau k_{1}k_{2}\cdots k_{n}}=(-1)^{\tau j_{1}j_{2}\cdots j_{n}}.$$
\end{corollary}
\begin{proof}
    When we move an element $a_{ik}$, the $\tau(i)$ and $\tau(k)$ change the parity simutaneously and 
    theri sum's parity keeps unchanged.
    After finite transposition, we get 
    $$(-1)^{\tau(i_{1}i_{2}\cdots i_{n})+\tau (k_{1}k_{2}\cdots k_{n})}=(-1)^{\tau(12\cdots n)+\tau(j_{1}j_{2}\cdots j_{n})=(-1)^{\tau(j_{1}j_{2}\cdots j_{n})}}.$$
    And it's obviously that we can have the same result if we have the expasion by column.
\end{proof}


\begin{theorem}
Any n-permutation can be transposed from $(123...n)$ and the times 
of transposition equals to the inversion number of the permutation.

\end{theorem}

\section{N-Order Determinant}
\subsection{Definition}
\subsubsection{n-order Determinant}
The n-order determinant is a scalar value that can be computed from 
the elements of a square matrix of size $n\times n$.

Actually, it can be written abstract
\begin{align*}
\det |A|  
& =
\begin{vmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    a_{21} & a_{22} & \cdots & a_{2n}\\
    \cdots & \cdots & \cdots & \cdots\\
    a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix} \\
& = \sum_{j_{1}j_{2}\cdots j_{n}}(-1)^{\tau (j_{1}j_{2}\cdots j_{n})} a_{1j_{1}}a_{2j_{2}}\cdots a_{nj_{n}} \\
& = \sum_{i_{1}i_{2}\cdots i_{n}}(-1)^{\tau (i_{1}i_{2}\cdots i_{n})} a_{i_{1}1}a_{i_{2}2}\cdots a_{i_{n}n} \\
& = a_{i1}A_{i1}+a_{i2}A_{i2}+\cdots+a_{in}A_{in}
\end{align*}

\subsubsection{Minor}
The minor of an element in a matrix is the determinant 
of the submatrix formed by deleting the 
row and column that contain the element.
That is 
$$M_{ij}=
\begin{vmatrix}
    a_{11} & a_{12} & \cdots & a_{1,j-1} & a_{1,j+1} & \cdots & a_{1n}\\
    \cdots &  &  & \cdots\\
    a_{i-1,1} & a_{i-1,2} & \cdots & a_{i-1,j-1} & a_{i-1,j+1} & \cdots & a_{in}\\
    \cdots &  &  & \cdots\\
    a_{i+1,1} & a_{i+1,2} & \cdots & a_{i+1,j-1} & a_{i+1,j+1} & \cdots & a_{i+1,n}\\
    \cdots &  &  & \cdots\\
    a_{n1} & a_{n2} & \cdots & a_{n,j-1} & a_{n,j+1} & \cdots & a_{nn}
\end{vmatrix}$$


\subsection{Expansion along a row or a column}
\begin{theorem}[Laplace Expansion Theorem]
    The determinant of a n-order matrix equals to any row's or column's 
    element multiplied by its algebraic cofactor.
    , like
    $$\det = \sum_{j=1}^{n}a_{ij}A_{ij}\ \text{for}\ i=1,2,\cdots,n = \sum_{i=1}^{n}a_{ij}A_{ij}\ \text{for}\ j=1,2,\cdots,n.$$
\end{theorem}

\begin{proof}
    The factor is 
    $$(-1)^{i+j}(-1)^{\tau (j_{1}j_{2}\cdots j_{i-1}j_{i+1}\cdots j_{n})}\ where\ (j_{1}j_{2}\cdots j_{i-1}j_{i+1}\cdots j_{n})\ is\ a\ permutation\ of\ 
    (12\cdots j-1,j+1\cdots n).$$
    then 
    \begin{align*}
        &(-1)^{i+j}(-1)^{\tau(j_{1}j_{2}\cdots j_{i-1}j_{i+1}\cdots j_{n})}\\
        &=(-1)^{(i-1)+(j-1)+\tau(j_{1}j_{2}\cdots j_{i-1}j_{i+1}\cdots j_{n})}\\
        &=(-1)^{\tau(i12\cdots i-1,i+1\cdots n)+(j-1)+\tau(j_{1}j_{2}\cdots j_{i-1}j_{i+1}\cdots j_{n})}\\
        &=(-1)^{\tau(i12\cdots i-1,i+1\cdots n)+\tau(jj_{1}j_{2}\cdots j_{i-1}j_{i+1}\cdots j_{n})}
        &=(-1)^{\tau(12\cdots i-1,i,i+1\cdots n)+\tau(j_{1}j_{2}\cdots j_{i-1}j_{i}j_{i+1}\cdots j_{n})}
        &=(-1)^{\tau(j_{1}j_{2}\cdots j_{i-1}jj_{i+1}\cdots j_{n})}
    \end{align*}
    Then the $$a_{ij}A_{ij}=(-1)^{j_{1}j_{2}\cdots j_{n}}a_{1j_{1}}a_{2j_{2}}\cdots a_{nj_{n}}$$ is a part of the determinant.
\end{proof}


\begin{theorem}
    The sum of the product of any one row's (column's) elements and another row's (column's) elements' algebraic cofactor is 0.
    That is $$\sum_{k=1}^{n}a_{ik}A_{jk}=0\ for\ i\neq j$$ or $$\sum_{k=1}^{n}a_{ki}A_{kj}=0\ for\ i\neq j.$$
\end{theorem}

\begin{proof}
    Construct a matrix of its form and it's obvious that for two rows or two columns are the same, the determinant is zero.
\end{proof}


Then we can get an important formula
$$\sum_{k=1}^{n}a_{ik}A_{jk}=\begin{cases}D\ j=i\\0\ j\neq i\end{cases}$$
and 
$$\sum_{k=1}^{n}a_{ki}A_{kj}=\begin{cases}D\ j=i\\0\ j\neq i\end{cases}.$$




\subsubsection{Algebraic Cofactor(Cofactor)}

The algebraic cofactor of an element in a matrix is the product of the minor of the element and 
$(-1)^{i+j}$, where $i$ is the row number and $j$ is the column number of the element. 
We denote it by 
$$A_{ij}=(-1)^{i+j}M_{ij}.$$
\subsubsection{k-minor  k-cofactor and k-algebraic cofactor}

A k-minor of a matrix is the determinant of a 
square submatrix obtained by \textbf{selecting} $k$ 
rows and $k$ columns from the original $n×n$ 
matrix, where $n$ is the dimension of the matrix.

A k-order principle minor is the determinant of a 
square submatrix obtained by \textbf{selecting} $k$
rows and $k$ columns from the original $n×n$
matrix, where $n$ is the dimension of the matrix and 
$i_{l}=j_{l}\ for\ l=1,2,\cdots,k$. 



A k-cofactor of a matrix is a determinant that the original determinant deleting the 
k-minor that remains as the follow order.

A k-algebraic cofactor of a matrix is a product of $(-1)^{i_{1},i_{2},\cdots,i_{k};j_{1},j_{2},\cdots,j_{k}}$
 and the k-cofactor.
\subsection{Properties}
\begin{enumerate}
    \item Transcope the matrix, the determinant does not change.
    \item 
    $$\begin{vmatrix}
        a_{11} & a_{12} & \cdots & a_{1n}\\
        \cdots &  &  & \cdots\\
        ka_{i1} & ka_{i2} & \cdots & ka_{in}\\
        \cdots &  &  & \cdots\\
        a_{n1} & a_{n2} & \cdots & a_{nn}
        \end{vmatrix}
    = k\begin{vmatrix}
        a_{11} & a_{12} & \cdots & a_{1n}\\
        \cdots &  &  & \cdots\\
        a_{i1} & a_{i2} & \cdots & a_{in}\\
        \cdots &  &  & \cdots\\
        a_{n1} & a_{n2} & \cdots & a_{nn}
        \end{vmatrix}$$
    \item 
    $$\begin{vmatrix}
        a_{11} & a_{12} & \cdots & a_{1n}\\
        \cdots &  &  & \cdots\\
        b_{1}+c_{1} & b_{2}+c_{2} & \cdots & b_{n}+c_{n}\\
        \cdots &  &  & \cdots\\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{vmatrix}
    =
    \begin{vmatrix}
        a_{11} & a_{12} & \cdots & a_{1n}\\
        \cdots &  &  & \cdots\\
        b_{1} & b_{2} & \cdots & b_{n}\\
        \cdots &  &  & \cdots\\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{vmatrix}
    +
    \begin{vmatrix}
        a_{11} & a_{12} & \cdots & a_{1n}\\
        \cdots &  &  & \cdots\\
        c_{1} & c_{2} & \cdots & c_{n}\\
        \cdots &  &  & \cdots\\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{vmatrix}$$
    \item If there are two rows or columns that are the same, the determinant is 0.
    \item If there are two rows or columns are proportionable, the determinant is 0.
    \item Add a row's or a column's k-times into another one, the determinant keeps the same. 
    \item Exchange two rows or columns, the determinant changes its sign.
\end{enumerate}

\textbf{Proof.}
\begin{enumerate}
    \item 
If we transcope the determinant,
$$
\begin{vmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    a_{21} & a_{22} & \cdots & a_{2n}\\
    \cdots &  &  & \cdots\\
    a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}
\ \rightarrow\ 
\begin{vmatrix}
    a_{11} & a_{21} & \cdots & a_{n1}\\
    a_{12} & a_{22} & \cdots & a_{n2}\\
    \cdots &  &  & \cdots\\
    a_{1n} & a_{2n} & \cdots & a_{nn}
\end{vmatrix}$$
like this, then we expand the right one with respect to the rows like this 
$$ \sum_{i_{1}i_{2}\cdots i_{n}}(-1)^{\tau (i_{1}i_{2}\cdots i_{n})} a_{i_{1}1}a_{i_{2}2}\cdots a_{i_{n}n}$$
Actually it keeps from the left one.\\

\item 
\begin{align*}
&\begin{vmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    \cdots &  &  & \cdots\\
    ka_{i1} & ka_{i2} & \cdots & ka_{in}\\
    \cdots &  &  & \cdots\\
    a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}\\
& = k \left( a_{i1}A_{i1} + a_{i2}A_{i2} + \cdots + a_{in}A_{in} \right) \\
& = k \begin{vmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    \cdots &  &  & \cdots\\
    a_{i1} & a_{i2} & \cdots & a_{in}\\
    \cdots &  &  & \cdots\\
    a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}
\end{align*}

\item
\begin{align*}
\begin{vmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    \cdots &  &  & \cdots\\
    b_{1}+c_{1} & b_{2}+c_{2} & \cdots & b_{n}+c_{n}\\
    \cdots &  &  & \cdots\\
    a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}
& = (b_{1}+c_{1})A_{i1} + (b_{2}+c_{2})A_{i2} + \cdots + (b_{n}+c_{n})A_{in} \\
& = (b_{1}A_{i1} + b_{2}A_{i2} + \cdots + b_{n}A_{in}) + (c_{1}A_{i1} + c_{2}A_{i2} + \cdots + c_{n}A_{in}) \\
& = \begin{vmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    \cdots &  &  & \cdots\\
    b_{1} & b_{2} & \cdots & b_{n}\\
    \cdots &  &  & \cdots\\
    a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}
+ \begin{vmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    \cdots &  &  & \cdots\\
    c_{1} & c_{2} & \cdots & c_{n}\\
    \cdots &  &  & \cdots\\
    a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}
\end{align*}

\item    Assume that the l-th and k-th rows are the same,
$det = \sum_{j_{1}j_{2}\cdots j_{n}}(-1)^{\tau (j_{1}j_{2}\cdots j_{n})}a_{1j_{1}a_{2j_{2}}\cdots a_{nj_{n}}}$
Since $a_{li}=a_{ki}$ for $i=1,2,\cdots,n$, then $a_{lj_{l}}a_{kj_{k}}=a_{lj_{k}}a_{kj_{l}}$ and the parity changes.
The sum is zero.
\item 
Use \textbf{2} and \textbf{4}

\item 
Use \textbf{3} and \textbf{5}

\item Obviously.

\end{enumerate}


\begin{pro}
    For n-square matrix $A$,
    \begin{enumerate}
        \item $\det (kA)=k^{n}\det A$
        \item $\det A^{T}=\det A$
        \item $\det A^{-1}=(\det A)^{-1}$
        \item $\det \begin{bmatrix}A&0\\C&D\end{bmatrix}=\det A\det D=\det\begin{bmatrix}A&B\\0&D\end{bmatrix}$
        \item If $A$ is orthogonal matrix, then $\det A=\pm1$
        \item If $A\xrightarrow{+cR_{i}} B$, then $\det B=c\det A$
        \item If $A\xrightarrow{R_{j}+kR_{i}} B$, then $\det B=\det A$
        \item If $A\xrightarrow{R_{ij}} B$, then $\det B=-\det A$
    \end{enumerate}
\end{pro}





\section{Application}
\subsection{Cramer Rule}
\begin{theorem}[Cramer Rule]

    If the system of linear equations' 
    $$Ax=b$$
    coefficent matrix 
    $$
    A=
    \begin{vmatrix}
        a_{11} & a_{12} & \cdots & a_{1n}\\
        \cdots &  &  & \cdots\\
        a_{i1} & a_{i2} & \cdots & a_{in}\\
        \cdots &  &  & \cdots\\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{vmatrix}
    $$
    its determinant is $d = |A| \neq 0$, then the system of 
    linear equations has solution, and the solution is unique, and can be expressed as 
    $$x_{1}=\frac{d_{1}}{d},\ x_{2}=\frac{d_{2}}{d},\ \cdots,\ x_{n}=\frac{d_{n}}{d}.$$
    where $d_{i}$ is the determinant of the matrix whose the j-th column is replaced by 
    the constant column vector $b$.
    $$d_{j}=
    \begin{vmatrix}
        a_{11} & \cdots & a_{1,j-1} & b_{1} &\cdots & a_{in}\\
        \cdots &  & \cdots & \cdots & \cdots\\
        a_{i1} & \cdots & a_{i,j-1} & b_{i} &\cdots & a_{in}\\
        \cdots &  & \cdots & \cdots & \cdots\\
        a_{n1} & \cdots & a_{n,j-1} & b_{n} &\cdots & a_{nn}
    \end{vmatrix}
    ,\ j=1,2,\cdots,n.$$
\end{theorem}
There are three results inside the theorem:
\begin{enumerate}
    \item The system of linear equations has solution.
    \item The solution is unique.
    \item The solution is expressed as the formula.
\end{enumerate}

\begin{proof}
\end{proof}

\begin{theorem}
    If the homogenous system of linear equations' coefficent matrix's 
    determinant $|A|\neq 0$, then it only has zero solution. On other word, if 
    the homogenous system of linear equations has non-zero solution, then $|A|=0$ is certainly.
\end{theorem}
\begin{proof}
    Using the Cramer Rule, we have $d_{j}=0,\ j=1,2,\cdots,n$. That is to say,
    $$(0,0,\cdots,0)$$ is its unique solution.
\end{proof}


\subsection{Product Rule}

\subsubsection{Elementary Matrix}

\begin{theorem}
    The determinant of the elementary matrix is not zero and 
    $$\det E_{ij}=-1\ \det E_{i}(c)=c\neq 0\ \det E_{ij}(k)=1.$$
\end{theorem}

\begin{theorem}
    If $P$ is an elementary matrix then 
    $$\det PA = \det P\det A.$$
\end{theorem}

\subsubsection{Product Rule}
\begin{theorem}[Product Rule]
    Suppose there are two matrix $A$ and $B$,
    and there determinant is $D_{1}=|A|,\ D_{2}=|B|$.
    Then the determinant of the product of $A$ and $B$ is
    $$C=D_{1}D_{2}.$$
\end{theorem}

\begin{proof}
    If $A$ is an invertible n-square, then $A$ can be expressed as the multiplication 
    of elementary metrices
    $$A=P_{s}P_{s-1}\cdots P_{1},$$
    thus $$\det A =\det P_{s}\det P_{s-1}\cdots \det P_{1}$$
    and $$\det AB = \det P_{s}\det P_{s-1}\cdots \det P_{1} \det B=\det A \det B= D_{1}D_{2}.$$

    If $A$ is not an invertible matrix, then the $r(A)<n$ and there exists a couples of $P,Q$ satisfying that 
    $$P_{l}P_{l-1}\cdots P_{1}AQ_{1}Q_{2}\cdots Q_{t}=diag(1,\cdots,1,0,\cdots,0):= \Lambda ,$$
    where the rank of $\Lambda$ is $r(A)$, then 
    $$A = P^{-1}_{1}P^{-1}_{2}\cdots P^{-1}_{l}\Lambda Q^{-1}_{t}Q^{-1}_{t-1}\cdots Q^{-1}_{1},$$
    then $$\det \Lambda\cdots =0$$
    $$\det A = 0$$
    $$\det AB = 0.$$
\end{proof}

\begin{corollary}
    If $A_{i}(i=1,2,\cdots,s)$ is n-square, then 
    $$\det (A_{1}A_{2}\cdots A_{s})=\det A_{1}A_{2}\cdots A_{s}.$$
\end{corollary}


\section{Adjoint Matrix}
\begin{defi}
    Set $A$ as $n\times n$ matrix, and set $A_{ij}$ as the 
    determinant of the algebraic cofactor of $a_{ij}$. Then the 
    $$
    A^{*}
    =\begin{bmatrix}
        A_{11} & A_{21} & \cdots & A_{n1}\\
        A_{12} & A_{22} & \cdots & A_{n2}\\
        \vdots & \vdots & \ddots & \vdots\\
        A_{1n} & A_{2n} & \cdots & A_{nn}
    \end{bmatrix}
    $$
    is called adjoint matrix of the matrix $A$.
\end{defi}
\begin{pro}
    If $A$ is a square matrix, $$AA^{*}=\det{A}I.$$
\end{pro}

\begin{proof}
    \begin{align*}
        A^{*}A
        &=
        \begin{bmatrix}
            A_{11} & A_{21} & \cdots & A_{n1}\\
            A_{12} & A_{22} & \cdots & A_{n2}\\
            \vdots & \vdots & \ddots & \vdots\\
            A_{1i} & A_{2i} & \cdots & A_{ni}\\
            \vdots & \vdots & \ddots & \vdots\\
            A_{1n} & A_{2n} & \cdots & A_{nn}
        \end{bmatrix}
        \begin{bmatrix}
            a_{11}&a_{12} & \cdots & a_{1j} & a_{1n}\\
            a_{21} & a_{22} & \cdots & a_{2j} & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots & \vdots\\
            a_{n1} & a_{2n} & \cdots & a_{nj} & a_{nn}
        \end{bmatrix}\\
        [A^{*}A]_{ij}&=
        a_{1j}A_{1i}+a_{2j}A_{2i}+\cdots+a_{nj}A_{nj}\\
        &=\sum_{k=1}^{n}a_{kj}A_{ki}\\
        &=\begin{cases}
            \det{A} & j=i\\
            0 & j\neq i
        \end{cases}\\
        A^{*}A&=
        \begin{bmatrix}
            \det{A} & 0 & 0 & \cdots & 0\\
            0 & \det{A} & 0 & \cdots & 0\\
            \vdots & \vdots & \ddots & \vdots & \vdots\\
            0 & 0 & 0 & \cdots & \det{A}
        \end{bmatrix}
        &=
        \det A I
    \end{align*}
\end{proof} 

\begin{theorem}
    A square matrix $A$ is invertible if and 
    only if its determinant is not equal to zero.
    When it is invertible,
    $$A^{-1}=\frac{1}{\det A}A^{*}.$$
\end{theorem}

\begin{proof}
    \textbf{Sufficiency:}
    If $\det A\neq 0$, then 
    $$(\frac{1}{\det A}A^{*})A=I$$
    and $A^{-1}=\frac{1}{\det A}A^{*}.$

    \textbf{Neccessity:}
    If $A$ is invertible, then $AA^{-1}=I$.
    $$\det AA^{-1}=\det{A} \det A^{-1}=1$$
    thus $\det A \neq 0$.
\end{proof}
 
\section{Determinant and the rank of a Matrix}
\begin{theorem}
    Set $A$ as a n-square, then $A$ is full rank if and only if
    $$\det A \neq 0.$$
\end{theorem}


\textbf{Note.}Three conditions following are equivalent.
\begin{enumerate}
    \item $A$ is full rank.
    \item $A$ is invertible.
    \item $\det A \neq 0$.
\end{enumerate}

\begin{theorem}
    Set $A$ as a $m\times n$ matrix, then $r(A)=r$ if
    and only if there exists a r-rank minor that is not zero,
    and all r+1-rank minor is zero.
\end{theorem}
\textbf{Note.} The rank of the matrix equals to the highest rank 
of the non-zero minor.

\begin{proof}




    \textbf{Neccessity:}
    If $r(A)=r$, then there the first $r$ rows of the matrix 
    is linear irrelative and the rank of the matrix
    $$\begin{bmatrix}
        a_{11}&a_{12} & \cdots & a_{1n}\\
        a_{21} & a_{22} & \cdots & a_{2n}\\
        \vdots & \vdots & \ddots & \vdots\\
        a_{r1} & a_{r2} & \cdots & a_{rn}
    \end{bmatrix}$$
    is $r$. Then the rank of its transcope is also m. Then its m-rank minor, 
    which is $r\times r$ square is not zero.

    Select any $r+1$ rows of the matrix, then the rank 
    of it is $r+1$, and its $r+1\times r+1$ square minor's rank 
    is r. Since $r<r+1$, the determinant of the $r+1\times n$ submatrix 
    is zero.

    Then for all rows larger than r+1, the determinant of the submatrix
    is zero and any k-rank minor is zero$(k>r)$.

    \textbf{Sufficiency:}
    If there exists a r-rank minor that is not zero,
    and all r+1-rank minor is zero. Assume that the rank 
    of the matrix is $k$, since r+1-rank minor is zero, from the \textbf{Neccessity}
    we know that $k<r+1$. Suppose $k<r$, then all the r-minor is zero, it contradicts 
    to the \textbf{Neccessity}. Thus, $k\leq r$, so $k=r$.
\end{proof}








\newpage
\section{Exercise}
\begin{exercise}[Vandermonde Determinant]
$$d=
\begin{vmatrix}
    1 & 1 & 1 & \cdots & 1\\
    a_{1} & a_{2} & a_{3} & \cdots & a_{n}\\
    a_{1}^{2} & a_{2}^{2} & a_{3}^{2} & \cdots & a_{n}^{2}\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    a_{1}^{n} & a_{2}^{n} & a_{3}^{n} & \cdots & a_{n}^{n}
\end{vmatrix}$$
is the n-order determinant of the Vandermonde matrix.
Now we prove that for any $n \geq 2$, d equals to the product of 
these n numbers all possible differences.
\end{exercise}
\begin{solution}
Use the method of induction, for $k=2$, we have
$$
\begin{vmatrix}
    1 & 1 \\
    a_{1} & a_{2}
\end{vmatrix}
=(a_{2}-a_{1}).$$
Assume that for $k=n-1$ the result keeps, then we
consider the $k=n$ case.
\begin{align*}
d_{n} 
& =
\begin{vmatrix}
    1 & 1 & 1 & \cdots & 1 \\
    a_{1} & a_{2} & a_{3} & \cdots & a_{n} \\
    a_{1}^{2} & a_{2}^{2} & a_{3}^{2} & \cdots & a_{n}^{2} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{1}^{n-1} & a_{2}^{n-1} & a_{3}^{n-1} & \cdots & a_{n}^{n-1}
\end{vmatrix}\\
& =
\begin{vmatrix}
    1 & 1 & 1 & \cdots & 1 \\
    0 & a_{2}-a_{1} & a_{3}-a_{1} & \cdots & a_{n}-a_{1} \\
    0 & a_{2}(a_{2}-a_{1}) & a_{3}(a_{3}-a_{1}) & \cdots & a_{n}(a_{n}-a_{1}) \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & a_{2}^{n-2}(a_{2}-a_{1}) & a_{3}^{n-2}(a_{3}-a_{1}) & \cdots & a_{n}^{n-2}(a_{n}-a_{1})
\end{vmatrix}\\
& =
\begin{vmatrix}
    a_{2}-a_{1} & a_{3}-a_{1} & \cdots & a_{n}-a_{1} \\
    a_{2}(a_{2}-a_{1}) & a_{3}(a_{3}-a_{1}) & \cdots & a_{n}(a_{n}-a_{1}) \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{2}^{n-2}(a_{2}-a_{1}) & a_{3}^{n-2}(a_{3}-a_{1}) & \cdots & a_{n}^{n-2}(a_{n}-a_{1})
\end{vmatrix}\\
& =
(a_{2}-a_{1})(a_{3}-a_{1})\cdots(a_{n}-a_{1})
\begin{vmatrix}
    1 & 1 & \cdots & 1 \\
    a_{2} & a_{3} & \cdots & a_{n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{2}^{n-2} & a_{3}^{n-2} & \cdots & a_{n}^{n-2}
\end{vmatrix}\\
& =
(a_{2}-a_{1})(a_{3}-a_{1})\cdots(a_{n}-a_{1})\times d_{n-1}
\end{align*}

Then the result holds for $k=n$. For simplicity, we write 
$$d_{n}=\prod_{1\leq i<j\leq n}(a_{j}-a_{i}).$$
\end{solution}

\begin{exercise}
    Assume that $A$ is a invertible 4-square matrix.
    $$
    A=
    \begin{bmatrix}
        a_{11} & a_{12} & a_{13} & a_{14}\\
        a_{21} & a_{22} & a_{23} & a_{24}\\
        a_{31} & a_{32} & a_{33} & a_{34}\\
        a_{41} & a_{42} & a_{43} & a_{44}
    \end{bmatrix}
    $$
    Find the solution of the 
    $$
    \begin{cases}
        a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}+a_{24}x_{4}=b_{2}\\
        a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}+a_{34}x_{4}=b_{3}\\
        a_{41}x_{1}+a_{42}x_{2}+a_{43}x_{3}+a_{44}x_{4}=b_{4}
    \end{cases}
    $$
\end{exercise}

\begin{solution}
    Actually, since 
    $$\sum_{j=1}^{4}a_{ij}A_{ij}=\begin{cases}
        \det A,\ i=j\\
        0,\ i\neq j\end{cases}$$
    we know that $(A_{11},A_{12},A_{13},A_{14})^{T}$ is a solution.
    Since $A$ is a invertible 4-square matrix, the rank of $A$ is 4 and 
    the coefficent matrix's rank is 3. Thus there is only one fundmental solution 
    vector.
    $$k(A_{11},A_{12},A_{13},A_{14})^{T}\ for\ k\in F$$
\end{solution}

\begin{exercise}
    Calculate the determinant 
    $$D_{n}
    \begin{vmatrix}
        a_{1}+b_{1}&a_{2}&\cdots&a_{n}\\
        a_{1}&a_{2}+b_{2}&\cdots&a_{n}\\
        \vdots&\vdots&\ddots&\vdots\\
        a_{1}&a_{2}&\cdots&a_{n}+b_{n}
    \end{vmatrix}
    ,\ b_{1}b_{2}\cdots b_{n}\neq 0.$$
\end{exercise}

\begin{solution}
    \begin{align*}
        D_{n}
        &=
        \begin{vmatrix}
            1&a_{1}&a_{2}&\cdots&a_{n}\\
            0&a_{1}+b_{1}&a_{2}&\cdots&a_{n}\\
            0&a_{1}&a_{2}+b_{2}&\cdots&a_{n}\\
            \vdots&\vdots&\vdots&\ddots&\vdots\\
            0&a_{1}&a_{2}&\cdots&a_{n}+b_{n}
        \end{vmatrix}\\
        &=
        \begin{vmatrix}
            1&a_{1}&a_{2}&\cdots&a_{n}\\
            -1&b_{1}&0&\cdots&0\\
            -1&0&b_{2}&\cdots&0\\
            \vdots&\vdots&\vdots&\ddots&\vdots\\
            -1&0&0&\cdots&b_{n}
        \end{vmatrix}\\ 
    \end{align*}
\end{solution}





\end{document}
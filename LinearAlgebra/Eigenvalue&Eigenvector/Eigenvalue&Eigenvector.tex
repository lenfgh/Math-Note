\documentclass{article}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[theorem]

\theoremstyle{definition} 
\newtheorem{defi}{Definition}[section]
\newtheorem{exa}{Example}[defi]
\newtheorem{exe}{Exercise}[section]
\newtheorem{sol}{Solution}[exe]
\newtheorem{pro}{Properties}[section]


\linespread{1.5}


\title{Eigenvalue and Eigenvector}
\author{Len Fu}
\date{11.27.2024}

\begin{document}

\maketitle

\begin{abstract}
    This is the note made by Len Fu during his learning progress in BIT.
    The main content is from \textit{Linear Algebra Done Right} and \textit{Linear Algebra Allenby}.
\end{abstract}

\tableofcontents

\newpage

\section{Similarity of the Matrix}
\subsection{Basis}
\begin{defi}
    Set $A,B\in C^{n\times n}$. If there exists
    an n-order invertible matrix $P$ such that 
    $$P^{-1}AP=B$$, we say that $A$ and $B$ are similar,
    denoted as $A\sim B$, and $P$ is called the 
    \textit{similarity transformation} from $A$ to $B$. 
\end{defi}

\begin{pro}[Reflectivity]
    $A\sim A$.
\end{pro}

\begin{pro}[Symmetry]
    If $A\sim B$, then $B\sim A$.
\end{pro}

\begin{pro}[Transitivity]
    If $A\sim B$ and $B\sim C$, then $A\sim C$.
\end{pro}



%%%%%%
\begin{pro}
\begin{enumerate}
\item $P^{-1}(A_{1}+A_{2}+\cdots+A_{n})P=P^{-1}A_{1}P+P^{-1}A_{2}P+\cdots+P^{-1}A_{n}P=P^{-1}\sum_{i=1}^{n}P^{-1}A_{i}P.$
\item $P^{-1}(kA)P=kP^{-1}AP.$
\end{enumerate}

\end{pro}














\subsection{Conditions of Similar Digonalizablity}


\begin{defi}[Digonalizable]
    If there exists an invertible matrix $P$ such that
    $$P^{-1}AP=D$$
    where $A$ is a square and $D$ is a diagonal matrix.
    Then $A$ is called \textit{diagonalizable}.
\end{defi}

\begin{theorem}
    A n-square $A$ is similar diagonalizable if and only if $A$ has $n$ linear irrelative eigenvectors.
\end{theorem}

\begin{proof}
\begin{align*}
P^{-1}AP=diag(\lambda_{1},\lambda_{2},\cdots,\lambda_{n})\\
AP=Pdiag(\lambda_{1},\lambda_{2},\cdots,\lambda_{n})
\end{align*}
Now set $P=[X_{1},X_{2},\cdots,X_{n}]$, and 
$$[AX_{1},AX_{2},\cdots,AX_{n}]=[\lambda_{1}X_{1},\lambda_{2}X_{2},\cdots,\lambda_{n}X_{n}]$$
then we have $$(A-\lambda_{i})X_{i}=0,\ for\ i=1,2,\cdots,n.$$
Since $P$ is invertible, we can find $n$ linear irrelative vectors $X_{1},X_{2},\cdots,X_{n}$.
And $X_{1},X_{2},\cdots,X_{n}$ are n linear irrelative eigenvectors of $A$ and $\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$ are 
eigenvalues of $A$.

Inversely, if $A$ has $n$ linear irrelative eigenvectors $X_{1},X_{2},\cdots,X_{n}$ with eigenvalues $\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$,
satisfying that 
$$AX_{i}=\lambda_{i}X_{i},\ for\ i=1,2,\cdots,n.$$
Set $P=[X_{1},X_{2},\cdots,X_{n}]$ and obviously $P$ is invertible, and 
$$P^{-1}AP=diag{\lambda_{1},\lambda_{2},\cdots,\lambda_{n}}$$
which reveals that $A$ is similar diagonalizable.
\end{proof}

\textbf{Note.}
\begin{enumerate}
\item The similar transformation matrix $P$ is not unique.
\item The order of $X_{1},X_{2},\cdots,X_{n}$ changes as the order of $\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$ changes.
\end{enumerate}


\begin{theorem}
The eigenvectors of $A$ with different eigenvalues are linearly independent.
\end{theorem}

\begin{proof}









\section{Eigenvalue and Eigenvector of the Matrix}
\subsection{Basis}\
\begin{defi}
    Set $A$ as a $n\times n$ square, if there exists a 
    number $\lambda$ and $n-nonzero$ vector $X$, satisfying 
    $$AX=\lambda X\ or\ (\lambda I-A)X=0$$
    then we say that $\lambda$ is an eigenvalue of $A$, and
    $X$ is an eigenvector of $A$ with eigenvalue $\lambda$.
\end{defi}

\textbf{Note.}
\begin{enumerate}
    \item Only squares have eigenvectors and eigenvalues.
    \item Eigenvector must be nonvector and eigenvalue can be zero.
\end{enumerate}


\begin{defi}
    $(\lambda I-A)$ is the eigrnmatrix of $A$. $|\lambda I-A|$ is the eigenpolynomial of $A$.
    $|\lambda I-A|=0$ is the eigenequation of the matrix $A$.
\end{defi}

Then the eigenvector of $A$ with eigenvalue $\lambda$ is the combination of the solution vectors of $(\lambda I-A)X=0$.

Since $(\lambda I-A)X=0$ and $X$ is nonzero vector, then 
$\det (\lambda I-A)$ should be zero to ensure $X$ is nonzero vector of the solution.

Consider the solution of $(\lambda I-A)X=0$.
The characteristic polynomial of $A$ is 
$$b_{n}\lambda^{n}+b_{n-1}\lambda^{n-1}+\cdots+b_{1}\lambda+b_{0}.$$

To solve the polynomial,
\begin{align*}
    &\begin{vmatrix}
        \lambda-a_{11} & -a_{12} & \cdots & -a_{1n} \\
        -a_{21} & \lambda-a_{22} & \cdots & -a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        -a_{n1} & -a_{n2} & \cdots & \lambda-a_{nn}
    \end{vmatrix}
    \\ & =
    b_{n}\lambda^{n}+b_{n-1}\lambda^{n-1}+\cdots+b_{1}\lambda+b_{0}
\end{align*}
Consider the expasion of the determinant, except for 
$$(\lambda-a_{11})(\lambda-a_{22})\cdots (\lambda-a_{nn})$$ 
other terms' highest order of $\lambda$ is $n-2$.
Then the coefficents
$$\begin{cases}
    b_{n} = 1 \\
    b_{n-1} = -(a_{11}+a_{22}+\cdots+a_{nn}) = tr()\\ 
\end{cases}
$$
And we divide the determinant into two parts and one is 
$$\begin{vmatrix}
    -a_{11} & -a_{12} & \cdots & -a_{1n} \\
    -a_{21} & \lambda-a_{22} & \cdots & -a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    -a_{n1} & -a_{n2} & \cdots & \lambda-a_{nn}
\end{vmatrix}$$
the other one doesn't contribute to the $b_{0}$, thus $$b_{0}=(-1)^{n}|A|.$$

From the polynomial theorem, 
$$f(\lambda)=(\lambda-\lambda_{1})(\lambda-\lambda_{2})\cdots (\lambda-\lambda_{n})=0.$$
Then 
$$b_{0}=(-1)^{n}\lambda_{1}\lambda_{2}\cdots\lambda_{n}\ b_{n-1}=-(\lambda_{1}+\lambda_{2}+\cdots+\lambda_{n})$$

And there are some properties of the \textbf{Eigenvalue and Eigenvector}.
\begin{pro}
\begin{enumerate}
    \item $|A|=\lambda_{1}\lambda_{2}\cdots\lambda_{n}$.
    \item $tr A=\sum_{i=1}^{n}\lambda_{i}$
    \item If $X_{1},X_{2},\cdots,X_{s}$ are eigenvectors of $A$ that belong to eigenvalue $\lambda_{0}$, then
    the linear combination of $X_{1},X_{2},\cdots,X_{s}$ is also an eigenvector of $A$ that belongs to eigenvalue $\lambda_{0}$.
    And all eigenvectors plus zero vector forms an \textbf{eigenspace} of $A$ with eigenvalue $\lambda_{0}$, denoted as $V_{\lambda_{0}}$
    and it's a solution space of $(\lambda_{0}I-A)X=0.$
\end{enumerate}
\end{pro}

\begin{pro}If $\lambda$ is an eigenvalue of $A$ with eigenvector $X$, then we have
    \begin{enumerate}

\item $k\lambda$ is the eigenvalue of $kA$.
\item $\lambda^{m}$ is the eigenvalue of $A^{m}$($m\in N^{*}$).
\item $f(\lambda)$ is the eigenvalue of $f(A)$ if $f$ is a polynomial transformation.
\item When $A$ is invertible, $\lambda^{-1}$ is the eigenvalue of $A^{-1}$
    \end{enumerate}
And $X$ is the eigenvector of matrices above with corresponding eigenvalue.

\end{pro}

\begin{pro}
The matrix $A$ and $A^{T}$ have the same \textbf{spectrum}.
\end{pro}







\section{Exercise}

\begin{exe}
The eigenvalues of $A$ are $1,2,3$, find the eigenvalues of $A^{2}-2I$.
\end{exe}

\begin{sol}
    We know that $A\sim diag(1,2,3)$ and then $A^{2}\sim diag(1,4,9)$.
    Then $(A^{2}-2I)\sim (diag(1,4,9)-2I)=diag(-1,2,7)$, then the eigenvalues 
    of $A^{2}-2I$ are $-1,2,7$.
\end{sol}

\begin{exe}
    Sovle the maxima of the $$f(x_{1},x_{2})=2x_{1}^{2}+2x_{1}x_{2}+2x_{2}^{2}$$
    under the constraint $x_{1}^{2}+x_{2}^{2}=1$ using \textbf{Lagrange Multipliers}.
\end{exe}

\begin{sol}
    Using Lagrange Multipliers,
    $$L(x_{1},x_{2})=2x_{1}^{2}+2x_{1}x_{2}+2x_{2}^{2}-\lambda(x_{1}^{2}+x_{2}^{2}-1)$$
    and 
    $$$$
\end{sol}


















\end{document}
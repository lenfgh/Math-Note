\documentclass{article}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{amsmath}
\usepackage{mathptmx}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{corollary}{Corollary}[theorem]

\theoremstyle{definition} 
\newtheorem{defi}{Definition}[section]
\newtheorem{exa}{Example}[defi]
\newtheorem{exe}{Exercise}[section]
\newtheorem{sol}{Solution}[exe]
\newtheorem{pro}{Properties}[section]


\linespread{1.25}


\title{Eigenvalue and Eigenvector}
\author{Len Fu}
\date{11.27.2024}

\begin{document}

\maketitle

% todo finish the abstract part.

\begin{abstract}
    This is the note made by Len Fu during his learning progress in BIT. The main content is from \textit{Linear Algebra Done Right} and \textit{Linear Algebra, J.K.Wan, BIT 2024 Fall}.
    
    The follow part is a summary of the note.
    \begin{enumerate}
        \item \textbf{Eigenvalues and Eigenvectors}
        \begin{itemize}
            \item The concept and properties of eigenvalues and eigenvectors, computation of eigenvalues and eigenvectors.
        \end{itemize}
        \item \textbf{Diagonalization of Matrices}
        \begin{itemize}
            \item Criteria for diagonalizability, the process of diagonalization.
        \end{itemize}
        \item \textbf{Diagonalization of Real Symmetric Matrices using Orthogonal Matrices}
        \begin{itemize}
            \item Properties of eigenvalues and eigenvectors of real symmetric matrices, the process of orthogonal similarity diagonalization.
        \end{itemize}
        \item \textbf{Jordan Canonical Form of Matrices}
        \begin{itemize}
            \item The method for finding the Jordan canonical form \( J \) and the similarity transformation matrix \( P \).
        \end{itemize}
    \end{enumerate}
\end{abstract}

\tableofcontents

\newpage

\section{Eigenvalue and Eigenvector of the Matrix}
\subsection{Basis}\
\begin{defi}
    Set $A$ as a $n\times n$ square, if there exists a 
    number $\lambda$ and $n-nonzero$ vector $X$, satisfying 
    $$AX=\lambda X\ or\ (\lambda I-A)X=0$$
    then we say that $\lambda$ is an eigenvalue of $A$, and
    $X$ is an eigenvector of $A$ with eigenvalue $\lambda$.
\end{defi}

\textbf{Note.}
\begin{enumerate}
    \item Only squares have eigenvectors and eigenvalues.
    \item Eigenvector must be nonvector and eigenvalue can be zero.
\end{enumerate}


\begin{defi}
    $(\lambda I-A)$ is the eigrnmatrix of $A$. $|\lambda I-A|$ is the eigenpolynomial of $A$.
    $|\lambda I-A|=0$ is the eigenequation of the matrix $A$.
\end{defi}

Then the eigenvector of $A$ with eigenvalue $\lambda$ is the combination of the solution vectors of $(\lambda I-A)X=0$.

Since $(\lambda I-A)X=0$ and $X$ is nonzero vector, then 
$\det (\lambda I-A)$ should be zero to ensure $X$ is nonzero vector of the solution.

Consider the solution of $(\lambda I-A)X=0$.
The characteristic polynomial of $A$ is 
$$b_{n}\lambda^{n}+b_{n-1}\lambda^{n-1}+\cdots+b_{1}\lambda+b_{0}.$$

To solve the polynomial,
\begin{align*}
    &\begin{vmatrix}
        \lambda-a_{11} & -a_{12} & \cdots & -a_{1n} \\
        -a_{21} & \lambda-a_{22} & \cdots & -a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        -a_{n1} & -a_{n2} & \cdots & \lambda-a_{nn}
    \end{vmatrix}
    \\ & =
    b_{n}\lambda^{n}+b_{n-1}\lambda^{n-1}+\cdots+b_{1}\lambda+b_{0}
\end{align*}
Consider the expasion of the determinant, except for 
$$(\lambda-a_{11})(\lambda-a_{22})\cdots (\lambda-a_{nn})$$ 
other terms' highest order of $\lambda$ is $n-2$.
Then the coefficents
$$\begin{cases}
    b_{n} = 1 \\
    b_{n-1} = -(a_{11}+a_{22}+\cdots+a_{nn}) = tr()\\ 
\end{cases}
$$
And we divide the determinant into two parts and one is 
$$\begin{vmatrix}
    -a_{11} & -a_{12} & \cdots & -a_{1n} \\
    -a_{21} & \lambda-a_{22} & \cdots & -a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    -a_{n1} & -a_{n2} & \cdots & \lambda-a_{nn}
\end{vmatrix}$$
the other one doesn't contribute to the $b_{0}$, thus $$b_{0}=(-1)^{n}|A|.$$

From the polynomial theorem, 
$$f(\lambda)=(\lambda-\lambda_{1})(\lambda-\lambda_{2})\cdots (\lambda-\lambda_{n})=0.$$
Then 
$$b_{0}=(-1)^{n}\lambda_{1}\lambda_{2}\cdots\lambda_{n}\ b_{n-1}=-(\lambda_{1}+\lambda_{2}+\cdots+\lambda_{n})$$

And there are some properties of the \textbf{Eigenvalue and Eigenvector}.
\begin{pro}
\begin{enumerate}
    \item $|A|=\lambda_{1}\lambda_{2}\cdots\lambda_{n}$.
    \item $tr A=\sum_{i=1}^{n}\lambda_{i}$
    \item If $X_{1},X_{2},\cdots,X_{s}$ are eigenvectors of $A$ that belong to eigenvalue $\lambda_{0}$, then
    the linear combination of $X_{1},X_{2},\cdots,X_{s}$ is also an eigenvector of $A$ that belongs to eigenvalue $\lambda_{0}$.
    And all eigenvectors plus zero vector forms an \textbf{eigenspace} of $A$ with eigenvalue $\lambda_{0}$, denoted as $V_{\lambda_{0}}$
    and it's a solution space of $(\lambda_{0}I-A)X=0.$
\end{enumerate}
\end{pro}

\begin{pro}If $\lambda$ is an eigenvalue of $A$ with eigenvector $X$, then we have
    \begin{enumerate}

\item $k\lambda$ is the eigenvalue of $kA$.
\item $\lambda^{m}$ is the eigenvalue of $A^{m}$($m\in N^{*}$).
\item $f(\lambda)$ is the eigenvalue of $f(A)$ if $f$ is a polynomial transformation.
\item When $A$ is invertible, $\lambda^{-1}$ is the eigenvalue of $A^{-1}$
    \end{enumerate}
And $X$ is the eigenvector of matrices above with corresponding eigenvalue.

\end{pro}

\begin{pro}
The matrix $A$ and $A^{T}$ have the same \textbf{spectrum}.
\end{pro}

\subsection{Algebraic Multiplicity and Geometric Multiplicity}

\begin{defi}[Geometric Multiplicity]
As for the eigenvalue $\lambda_{i}$ of $A$, its all eigenvectors are the nonzero-solutions of the equation 
$(\lambda_{i}I-A)X=0$. Thus the number of independent eigenvectors of $A$ with eigenvalues $\lambda_{i}$ is 
no more than $n-rank(\lambda_{i}-A)$. The number is the dimension of the eigenspace $V_{\lambda_{i}}$ and the number 
of the solution vectors of the fundamental system of solutions of $A$. We call this number as the \textbf{geometric multiplicity} of $\lambda_{i}$, equals to 
$$q_{i}=n-rank(\lambda_{i}I-A),\ i=1,2,\cdots,s.$$I
\end{defi}

\begin{defi}[Algebraic Multiplicity]
From the theorem of the polynomial, n-square $A$ on $\mathbb{C}$ can be divided 
$$f_{A}(\lambda)=(\lambda-\lambda_{1})^{p_{1}}(\lambda-\lambda_{2})^{p_{2}}\cdots(\lambda-\lambda_{s})^{p_{s}}$$
where $\lambda_{1},\lambda_{2},\cdots,\lambda_{s}$ are all distinct eigenvalues of $A$. We call $p_{i}$ the \textbf{algebraic multiplicity} of $\lambda_{i}$.
\end{defi}

\begin{theorem}
The geometric multiplicity of any eigenvalue $\lambda_{i}$ of $A$ is not larger than the algebraic multiplicity of $\lambda_{i}$.
\end{theorem}
\begin{proof}
    Set $$X_{i1},X_{i2},\cdots,X_{iq}$$
    as a basis of the eigenspace $V_{\lambda_{i}}$, we extent it into a basis of $\mathbb{C^{n}}$ :
    $$X_{i1},X_{i2},\cdots,X_{iq_{i}},Y_{1},Y_{2},\cdots,Y_{n-q_{i}},$$
    then we have 
    \begin{align*}
        &A[X_{i1},X_{i2},\cdots,X_{iq_{i}},Y_{1},Y_{2},\cdots,Y_{n-q_{i}}]\\
        =&[\lambda_{i}X_{i1},\lambda_{2}X_{i2},\cdots,X_{iq_{i}},AY_{1},AY_{2},\cdots,AY_{n-q_{i}}]\\
        =&[X_{i1},X_{i2},\cdots,X_{iq_{i}},Y_{1},Y_{2},\cdots,Y_{n-q_{i}}]\begin{bmatrix}
            \lambda_{i}I_{q_{i}}&*\\0&A_{1}\end{bmatrix}\\
    \end{align*}
    where $A_{1}$ is $n-q_{i}$ square. Now set $$P=[X_{i1},X_{i2},\cdots,X_{iq_{i}},Y_{1},Y_{2},\cdots,Y_{n-q_{i}}],$$ 
    obviously $P$ id invertible. Then 
    \begin{align*}
        AP=P\begin{bmatrix}
            \lambda_{i}I_{q_{i}}&*\\0&A_{1}\end{bmatrix}\\
        and\\
        A\sim\begin{bmatrix}
            \lambda_{i}I_{q_{i}}&*\\0&A_{1}\end{bmatrix}
    \end{align*}
    Thus
    \begin{align*}
        f_{A}(\lambda)=&\det \left(
        \lambda I    
        -
            \begin{bmatrix}
                \lambda_{i}I_{q_{i}}&*\\0&A_{1}
            \end{bmatrix}
        \right)
        =&\det \left(
            \begin{bmatrix}
                (\lambda-\lambda_{i})I_{q_{i}}&-*\\
                0&\lambda I_{n-q_{i}}-A_{1}
            \end{bmatrix}
        \right)
        =&(\lambda-\lambda_{i})^{q_{i}}\det (\lambda I_{n-q_{i}}-A_{1})
        =&(\lambda-\lambda_{i})^{q_{i}}f_{A_{i}}(\lambda)
    \end{align*}
    Thus $p_{i}\geq \ q_{i},\ i=1,2,...,s$.
\end{proof}


\section{Similarity of the Matrix}
\subsection{Basis}
\begin{defi}
    Set $A,B\in C^{n\times n}$. If there exists
    an n-order invertible matrix $P$ such that 
    $$P^{-1}AP=B$$, we say that $A$ and $B$ are similar,
    denoted as $A\sim B$, and $P$ is called the 
    \textit{similarity transformation} from $A$ to $B$. 
\end{defi}

\begin{pro}[Reflectivity]
    $A\sim A$.
\end{pro}

\begin{pro}[Symmetry]
    If $A\sim B$, then $B\sim A$.
\end{pro}

\begin{pro}[Transitivity]
    If $A\sim B$ and $B\sim C$, then $A\sim C$.
\end{pro}


\begin{pro}
\begin{enumerate}
\item $P^{-1}(A_{1}+A_{2}+\cdots+A_{n})P=P^{-1}A_{1}P+P^{-1}A_{2}P+\cdots+P^{-1}A_{n}P=P^{-1}\sum_{i=1}^{n}P^{-1}A_{i}P.$
\item $P^{-1}(kA)P=kP^{-1}AP.$
\end{enumerate}
\end{pro}










\subsection{Conditions of Similar Digonalizablity}


\begin{defi}[Digonalizable]
    If there exists an invertible matrix $P$ such that
    $$P^{-1}AP=D$$
    where $A$ is a square and $D$ is a diagonal matrix.
    Then $A$ is called \textit{diagonalizable}.
\end{defi}

\begin{theorem}[NS Condition]
    A n-square $A$ is similar diagonalizable if and only if $A$ has $n$ linear independent eigenvectors.
\label{theorem:NS}
\end{theorem}

\begin{proof}
\begin{align*}
P^{-1}AP=diag(\lambda_{1},\lambda_{2},\cdots,\lambda_{n})\\
AP=Pdiag(\lambda_{1},\lambda_{2},\cdots,\lambda_{n})
\end{align*}
Now set $P=[X_{1},X_{2},\cdots,X_{n}]$, and 
$$[AX_{1},AX_{2},\cdots,AX_{n}]=[\lambda_{1}X_{1},\lambda_{2}X_{2},\cdots,\lambda_{n}X_{n}]$$
then we have $$(A-\lambda_{i})X_{i}=0,\ for\ i=1,2,\cdots,n.$$
Since $P$ is invertible, we can find $n$ linear irrelative vectors $X_{1},X_{2},\cdots,X_{n}$.
And $X_{1},X_{2},\cdots,X_{n}$ are n linear irrelative eigenvectors of $A$ and $\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$ are 
eigenvalues of $A$.

Inversely, if $A$ has $n$ linear irrelative eigenvectors $X_{1},X_{2},\cdots,X_{n}$ with eigenvalues $\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$,
satisfying that 
$$AX_{i}=\lambda_{i}X_{i},\ for\ i=1,2,\cdots,n.$$
Set $P=[X_{1},X_{2},\cdots,X_{n}]$ and obviously $P$ is invertible, and 
$$P^{-1}AP=diag{\lambda_{1},\lambda_{2},\cdots,\lambda_{n}}$$
which reveals that $A$ is similar diagonalizable.
\end{proof}

\textbf{Note.}
\begin{enumerate}
\item The similar transformation matrix $P$ is not unique.
\item The order of $X_{1},X_{2},\cdots,X_{n}$ changes as the order of $\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$ changes.
\end{enumerate}


\begin{theorem}
The eigenvectors of $A$ with different eigenvalues are linearly independent.
\end{theorem}

\begin{proof}
Set $\lambda_{1},\lambda_{2},\cdots,\lambda_{m}$ are non-eqaul eigenvalues of $A$, $X_{1},X_{2},\cdots,X_{n}$ 
are eigenvectors of $A$ with eigenvalues $\lambda_{1},\lambda_{2},\cdots,\lambda_{m}$, then 
$$ AX_{i}=\lambda_{i}X_{i}\ for\ i=1,2,\cdots,m.$$
Now I use the mathematical induction to prove the above statement.

\textbf{Base Case:m=1} Since $X_{1}\neq 0$, $X_{1}$ is linear independent.

\textbf{Inductive Hypothesis:m=k-1} Suppose the statement is true for $m=k-1$.

\textbf{Inductive Step:m=k} Consider the condition that $m=k$, 
$$k_{1}X_{1}+k_{2}X_{2}+\cdots+k_{m}X_{m}=0$$

Then we left multiply $A$, we get 
$$k_{1}AX_{1}+k_{2}AX_{2}+\cdots+k_{m}AX_{m}=0.$$

Since $AX_{i}=\lambda_{i}X_{i}$, we get 
$$k_{1}\lambda_{1}X_{1}+k_{2}\lambda_{2}X_{2}+\cdots+k_{m}\lambda_{m}X_{m}=0.$$

We multiply $\lambda_{m}$ on the first equation and we get 
$$k_{1}\lambda_{m}X_{1}+k_{2}\lambda_{2}X_{2}+\cdots+k_{m}\lambda_{m}X_{m}=0.$$

Then
$$k_{1}(\lambda_{1}-\lambda_{m})X_{1}+k_{2}(\lambda_{2}-\lambda_{m})X_{2}+\cdots+k_{m-1}(\lambda_{m-1}-\lambda_{m})X_{m-1}=0,$$
and since $\lambda_{i}\neq\lambda_{j}$ and $X_{1},X_{2},\cdots,X_{m-1}$ are linearly independent, 
then $$k_{i}=0\ i=1,2,\cdots,m-1.$$
Correspondingly, $k_{m}=0$.

Thus $k_{1},k_{2},\cdots,k_{m}=0$, which reveals that $X_{1},X_{2},\cdots,X_{m}$ are linearly independent.
\end{proof}

\begin{corollary}
    If n-square $A$ has n distinct eigenvalues, then $A$ can be similar diagonalizable.
\end{corollary}

\begin{theorem}
Set $\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$ are distinct eigenvalues of $A$, $X_{i1},X_{i2},\cdots,X_{il_{i}}$ 
are linear independent eigenvectors of $A$ belonging to eigenvalue $\lambda_{i},i=1,2,\cdots,m$, then the vector set of $X_{11},X_{12},\cdots,X_{1l_{1}},\cdots,X_{m1},X_{m2},\cdots,X_{ml_{l}}$ is linear independent.
\end{theorem}




\begin{theorem}[NS Condition]
    Set $\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$ are distinct eigenvalues of $A$, $p_{i}\ and\ q_{i}$ are the algebraic multiplicity and geometric multiplicity of $\lambda_{i}$ respectively.
    Then $A$ can be similar diagonalizable if and only if
    $$p_{i}=q_{i}\ i=1,2,\cdots,s.$$
\end{theorem}

\begin{proof}
    As we know that $$f_{A}(\lambda)=(\lambda-\lambda_{i})^{q_{i}}f_{A_{1}}(\lambda)$$
    If $A$ is similar diagonalizable, then its eigenpolynomial can written as $(\lambda-\lambda_{i})^{p_{i}}...$, then $p_{i}=q_{i}$ for all $i$.
\end{proof}

\subsection{Method to Similar Diagonalize}

\begin{enumerate}
    \item Find all the eigenvalues of $A$, like $\lambda_{1},\lambda_{2},\cdots,\lambda_{s}$.
    \item For every eigenvalue $\lambda_{i}$, find the rank of $(\lambda_{i}I-A)$, and judge that whether $q_{i}=n-rank(\lambda_{i}I-A)$ equals to its $p_{i}$. If they are equivalent, then it is diagonalizable. If they are not equivalent, then $A$ is not diagonalizable.
    \item If $A$ is diagonalizable, then for every $\lambda_{i}$, solve the $(\lambda_{i}i-A)X=0$, and we can have a fundamental solve
$$X_{i1},X_{i2},\cdots,X_{iq_{i}},\ i=1,2,\cdots,s.$$
    \item Let $P=[X_{11},X_{12},\cdots,X_{1q_{1}},\cdots,X_{s1},X_{s2},\cdots,X_{sq_{s}}]$, then we can get $P^{-1}AP=D$, where 
    $$D=diag(\lambda_{1},\cdots,\lambda-{1},\cdots,\lambda_{s},\cdots,\lambda_{s})$$
    with $q_{i}$ $\lambda_{i}$.
\end{enumerate}




















\section{Real-Symmetric Matrix's Normalised Diagonalization}

\begin{defi}[Conjugate Matrix]

    Set matrix $A$ in the field $\mathbb{C}$, and $A^{\dagger}$ is the Conjugate Matrix of $A$,
    where $$a_{ij} = a_{ij}^{\dagger}.$$    
\end{defi}

\begin{pro}
    \begin{enumerate}
        \item $(A^{\dagger})^{\dagger}=A$
        \item $(A^{\dagger})^{T}=(A^{T})^{\dagger}$
        \item $(kA)^{\dagger}=k^{\dagger}A^{\dagger}$
        \item $(A+B)^{\dagger}=A^{\dagger}+B^{\dagger}$
        \item $(AB)^{\dagger}=A^{\dagger}B^{\dagger}$
        \item $((AB)^{T})^{\dagger}=(B^{T})^{\dagger}(A^{T})^{\dagger}$
        \item If $A$ is invertible, then $(A^{-1})^{\dagger}=(A^{\dagger})^{-1}$
        \item If $A$ is square, then $\det A^{\dagger}=(\det A)^{\dagger}$
        \item $rank(A) = rank (A^{\dagger})$
    \end{enumerate}
\end{pro}

\begin{theorem}
    The eigenvalue of real symmetric matrix is real. 
\end{theorem}

\begin{theorem}
    Eigenvectors with different eigenvalues of real symmetric matrix is orthogonal.
\end{theorem}

\begin{proof}
Set $\lambda_{1},\lambda_{2}$ are two distinct eigenvalues of real symmetric matrix $A$, their conresponding eigenvectors are 
$X_{1},X_{2}$, then 
$$AX_{i}=\lambda_{i}X_{i},\ for\ i=1,2.$$
To prove $X_{1}\bot X_{2}$, we need to show that $X_{2}^{T}X_{1}=0$. Then 
\begin{align*}
    X_{2}^{T}A^{T}=&\lambda_{2}X_{2}^{T}\\
    X_{2}^{T}A^{T}X_{1}=&\lambda_{2}X_{2}^{T}X_{1}\\
    (\lambda_{2}-\lambda_{1})(X_{2}^{T}X_{1})&=0
\end{align*}
Since $\lambda_{2}\neq\lambda_{1}$, then $X_{2}^{T}X_{1}=0$.
\end{proof}

\begin{theorem}
Set $\lambda_{0}$ is any eigenvalue of real symmetric matrix $A$, and $p$ and $q$ represent its algebraic and geometric multiplicity respectively, then
$p=q$.
\end{theorem}

\begin{theorem}
    For any n real symmetric matrix $A$, there exists a n orthogonal matrix $P$ and a diagonal matrix $D$ such that $A=PDP^{-1}$.
    where $$D=diag(\lambda_{1},\cdots,\lambda_{n})\ and\ P=[\beta_{1},\beta_{2},\cdots,\beta_{n}]\ where\ \beta\ is orthogonal vectors.$$
\end{theorem}

We can use Gram-Schmidt Orthogonalization to orthogonalize the vectors :
\begin{enumerate}
    \item $\beta_1 = \alpha_1$。
    \item $i = 2, 3, \ldots, m$  $\beta_i$ 
    \[
    \beta_i = \alpha_i - \sum_{j=1}^{i-1} \frac{(\alpha_i, \beta_j)}{(\beta_j, \beta_j)} \beta_j
    \]
    where $(\cdot, \cdot)$ is the inner product.
    \item Normalize $\beta_i$ ，and we get the normalised vectors $\gamma_i$：
    \[
    \gamma_i = \frac{\beta_i}{\|\beta_i\|}
    \]
\end{enumerate}



\section{}
\subsection{Jordan Canonical Form}
\begin{defi}[Jordan Block]
    Define a m upper triangular matrix, as a m Jordan Block 
$$\begin{bmatrix}
    a&1&0\cdots&0&0&0\\
    0&a&1&\cdots&0&0\\
    \vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
    0&0&0&\cdots&a&1\\
    0&0&0&\cdots&1&a
\end{bmatrix}$$
    
\end{defi}

\begin{defi}[Jordan Form]
    Call a quasi-diagonal matrix $A$ as a Jordan Form if it can be written as
    \[
\begin{bmatrix}
J_1 & & \\
& J_2 & \\
& & \ddots \\
& & & J_s
\end{bmatrix}
\]
where $J_i$ is a Jordan Block.
\end{defi}

\begin{theorem}
    Set $A\in C^{n\times n}$, then $A$ can be similar to a Jordan Form.
    \[
    J=\begin{bmatrix}
        J_1 & & \\
        & J_2 & \\
        & & \ddots \\
        & & & J_s
    \end{bmatrix}
    \]
    and except for $J_1,J_2,\cdots,J_s$ 's order, $J$ is unique for $A$ and is Jordan Canonical Form of $A$.
\end{theorem}

\subsubsection{Elementary Divisors}
\begin{defi}[Elementary Transformation]
    Set $A$ as an n square, and let $A(\lambda)=\lambda I-A$, we call these three transformations as Elementary Transformation.:
    \begin{enumerate}
        \item Swap two rows or columns of $A(\lambda)$.
        \item Multiply a row or column of $A(\lambda)$ by a nonzero scalar.
        \item Use a polynomial of $\lambda$ to multiply a row or column of $A(\lambda)$ and add it to another row or column of $A(\lambda)$.
    \end{enumerate}
\end{defi}

\begin{defi}[Elementary Divisors]
    Set $A$ as a complex n-square, use elementary transformation to transform $A(\lambda)$ to diagonal matrix, Then decompose the diagonal elements into a product of distinct first-degree polynomial powers of 
    $\lambda$. The powers of all first-degree factors with exponents greater than zero are called the elementary divisors of $A$. 
\end{defi}

\begin{corollary}[m-Jordan Block]
    $$\begin{bmatrix}
        a&1&0\cdots&0&0\\
        0&a&1&\cdots&0\\
        \vdots&\vdots&\vdots&\ddots&\vdots\\
        0&0&0&\cdots&a\\
    \end{bmatrix}$$ 
    The elementary divisors of $A$ are $(\lambda-a)^m.$
\end{corollary}

\begin{theorem}
    Set $A$'s elementary divisors are $(\lambda-a_1)^{m_1},\cdots,(\lambda-a_s)^{m_s}$, then $A$'s Jordan Canonical Form is
    $$\begin{bmatrix}
        J_1&0&\cdots&0\\
        0&J_2&\cdots&0\\
        0&0&\ddots&0\\
        0&0&\cdots&J_s\\
    \end{bmatrix}$$
    where $J_i$ is 
    $$\begin{bmatrix}
        a_i&1&0\cdots&0\\
        0&a_i&1&\cdots&0\\
        \vdots&\vdots&\vdots&\ddots&\vdots\\
        0&0&0&\cdots&a_i\\
    \end{bmatrix}_{m_i\times m_i}\ (i=1,2,\cdots,s)$$
\end{theorem}










\subsection{Smith Normal Form}








\newpage
\section{Exercise}

\begin{exe}
The eigenvalues of $A$ are $1,2,3$, find the eigenvalues of $A^{2}-2I$.
\end{exe}

\begin{sol}
    We know that $A\sim diag(1,2,3)$ and then $A^{2}\sim diag(1,4,9)$.
    Then $(A^{2}-2I)\sim (diag(1,4,9)-2I)=diag(-1,2,7)$, then the eigenvalues 
    of $A^{2}-2I$ are $-1,2,7$.
\end{sol}

\begin{exe}
    Sovle the maxima of the $$f(x_{1},x_{2})=2x_{1}^{2}+2x_{1}x_{2}+2x_{2}^{2}$$
    under the constraint $x_{1}^{2}+x_{2}^{2}=1$ using \textbf{Lagrange Multipliers}.
\end{exe}

\begin{sol}
    Using Lagrange Multipliers,
    $$L(x_{1},x_{2})=2x_{1}^{2}+2x_{1}x_{2}+2x_{2}^{2}-\lambda(x_{1}^{2}+x_{2}^{2}-1)$$
    and 
  
\end{sol}


















\end{document}